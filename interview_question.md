# 面試題整理

### 有監督學習和無監督學習的區別

* 有監督學習：對具有標記的訓練樣本進行學習，以盡可能對訓練樣本集外的數據進行分類預測。（LR,SVM,BP,RF,GBRT）
* 無監督學習：對未標記的樣本進行訓練學習，比發現這些樣本中的結構知識。 (KMeans,DL)

### Regularization

Regularization 是針對 over-fitting 而提出的，以為在求解模型最優的是一般優化最小的經驗風險，現在在該經驗風險上加入模型複雜度這一項（ Regularization 項是模型參數向量的範數），並使用一個rate比率來權衡模型複雜度與以往經驗風險的權重，如果模型複雜度越高，結構化的經驗風險會越大，現在的目標就變為了結構經驗風險的最優化，可以防止模型訓練過度複雜，有效的降低過擬合的風險。

````
奧卡姆剃刀原理，能夠很好的解釋已知數據並且十分簡單才是最好的模型。
````

### over fitting

如果一味的去提高訓練數據的預測能力，所選模型的複雜度往往會很高，這種現象稱為 over fitting。所表現的就是模型訓練時候的誤差很小，但在測試的時候誤差很大。

##### 產生的原因

1. 因為參數太多，會導致我們的模型複雜度上升，容易過擬合
2. 權值學習迭代次數足夠多(Overtraining), 擬合了訓練數據中的噪聲和訓練樣例中沒有代表性的特徵.

##### 解決方法

1. 交叉驗證法
2. 減少特徵
3. 正則化
4. 權值衰減
5. 驗證數據

### generalization

generalization 是指模型對未知數據的預測能力

### 生成模型和判別模型

1. 生成模型：由數據學習聯合概率分佈P(X,Y)，然後求出條件概率分佈P(Y|X)作為預測的模型，即生成模型：P(Y|X)= P(X,Y) / P(X)。 

````
樸素貝葉斯 
生成模型可以還原聯合概率分佈p(X,Y)，並且有較快的學習收斂速度，還可以用於隱變量的學習
````
2. 判別模型：由數據直接學習決策函數Y=f(X)或者條件概率分佈P(Y|X)作為預測的模型，即判別模型。 

````
kNN、決策樹
直接面對預測，往往準確率較高，直接對數據在各種程度上的抽象，所以可以簡化模型
````

### 線性分類器與非線性分類器的區別以及優劣

如果模型是參數的線性函數，並且存在線性分類面，那麼就是線性分類器，否則不是。 
常見的線性分類器有：LR,貝葉斯分類，單層感知機、線性回歸常見的非線性分類器：決策樹、RF、GBDT、多層感知機

````
SVM 兩種都有 ( 要看是哪一種 core )
````

* 線性分類器速度快、開發方便，但是可能擬合效果不會很好
* 非線性分類器開發複雜，但是效果擬合能力強

##### 特徵比數據量還大時，選擇什麼樣的分類器？
````
線性分類器，因為維度高的時候，數據一般在維度空間裡面會比較稀疏，很有可能線性可分
````
##### 對於維度很高的特徵，你是選擇線性還是非線性分類器？
````
理由同上
````
##### 對於維度極低的特徵，你是選擇線性還是非線性分類器？
````
非線性分類器，因為低維空間可能很多特徵都跑到一起了，導致線性不可分
````

### ill-condition病態問題
訓練完的模型測試樣本稍作修改就會得到差別很大的結果，就是病態問題  
就是不能用的意思

### L1和L2正則的區別，如何選擇L1和L2正則

````
他們都是可以防止過擬合，降低模型複雜度
````

* L1是在loss function後面加上模型參數的1範數（也就是|xi|）
* L2是在loss function後面加上模型參數的2範數（也就是sigma(xi^2)），注意L2範數的定義是sqrt(sigma(xi^2))，在正則項上沒有添加sqrt根號是為了更加容易優化
* L1 會產生稀疏的特徵
* L2 會產生更多地特徵但是都會接近於0

L1會趨向於產生少量的特徵，而其他的特徵都是0，而L2會選擇更多的特徵，這些特徵都會接近於0。 L1在特徵選擇時候非常有用，而L2就只是一種規則化而已。


### 特徵向量的歸一化方法

1. 線性函數轉換，表達式如下：y=(x-MinValue)/(MaxValue-MinValue)
2. 對數函數轉換，表達式如下：y=log10 (x)
3. 反餘切函數轉換，表達式如下：y=arctan(x)*2/PI
4. 減去均值，乘以方差：y=(x-means)/ variance

### 特徵向量的異常值處理

1. 用均值或者其他統計量代替

### 越小的參數說明模型越簡單

過擬合的，擬合會經過曲面的每個點，也就是說在較小的區間裡面可能會有較大的曲率，這裡的導數就是很大，線性模型裡面的權值就是導數，所以越小的參數說明模型越簡單。

````
也可以用 VC dimension 說明
````

### svm 中 rbf 核函數與高斯和函數的比較

高斯核函數好像是RBF核的一種

### KMeans初始類簇中心點的選取

選擇批次距離盡可能遠的K個點

````
首先随機選取一個點作為初始點，然後選擇距離與該點最遠的那個點作為中心點，再選擇距離與前兩個點最遠的點作為第三個中心點，直到選取大 k 個
````

選用層次聚類或者 Canopy 算法進行初始聚類

### ROC、AUC

ROC和AUC通常是用來評價一個二值分類器的好壞

ROC 具體概念是我們對每個可能的 classification threshold 進行評估，並觀察相應 classification threshold 的 True Positive Rate ( Recall ) 和 False Positive Rate

然後我們借助曲線下的面積 ( AUC: Area Under the ROC Curve
 ) 來找出那一個點，不然直接從線上找出那一點太沒效率了。
 
````
AUC 越大說明分類效果越好
````

##### 為什麼要使用ROC和AUC
因為當測試集中的正負樣本發生變化時，ROC曲線能基本保持不變，但是 precision 和 recall 可能就會有較大的波動。 

### 測試集和訓練集的區別
訓練集用於建立模型,測試集評估模型的預測等能力

### 優化 Kmeans
使用 kd 樹或者 ball tr​​ee
將所有的觀測實例構建成一顆 kd 樹，之前每個聚類中心都是需要和每個觀測點做依次距離計算，現在這些聚類中心根據 kd 樹只需要計算附近的一個局部區域即可

### 數據挖掘和機器學習的區別

機器學習是數據挖掘的一個重要工具，但是數據挖掘不僅僅只有機器學習這一類方法，還有其他很多非機器學習的方法，比如圖挖掘，頻繁項挖掘等。 感覺數據挖掘是從目的而言的，但是機器學習是從方法而言的。



